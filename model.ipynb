{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征工程集合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import learning_curve,validation_curve\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold,KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error\n",
    "from sklearn.base import clone\n",
    "\n",
    "from sklearn.linear_model import LinearRegression,Lasso,Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor,RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor,LGBMClassifier,callback\n",
    "from catboost import CatBoostRegressor,CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 490530 entries, 0 to 490529\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   pid     490530 non-null  object\n",
      " 1   label   490530 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 7.5+ MB\n"
     ]
    }
   ],
   "source": [
    "sample_submit = pd.read_csv('./data/提交示例.csv',encoding='utf-8')\n",
    "sample_submit.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(data):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = data.memory_usage().sum() \n",
    "    \n",
    "    for col in data.columns:\n",
    "        col_type = data[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = data[col].min()\n",
    "            c_max = data[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    data[col] = data[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    data[col] = data[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    data[col] = data[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    data[col] = data[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    data[col] = data[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    data[col] = data[col].astype(np.float32)\n",
    "                else:\n",
    "                    data[col] = data[col].astype(np.float64)\n",
    "        else:\n",
    "            data[col] = data[col].astype('category')\n",
    "    end_mem = data.memory_usage().sum() \n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = reduce_mem_usage(pd.read_csv('./data/train.csv',encoding='utf-8'))\n",
    "test_data = reduce_mem_usage(pd.read_csv('./data/test.csv',encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 209470 entries, 0 to 209469\n",
      "Data columns (total 8 columns):\n",
      " #   Column       Non-Null Count   Dtype   \n",
      "---  ------       --------------   -----   \n",
      " 0   pid          209470 non-null  category\n",
      " 1   label        209470 non-null  int8    \n",
      " 2   brand        209470 non-null  category\n",
      " 3   model        209470 non-null  category\n",
      " 4   province     209470 non-null  category\n",
      " 5   city         209470 non-null  category\n",
      " 6   enum_tag     209470 non-null  category\n",
      " 7   numeric_tag  189629 non-null  category\n",
      "dtypes: category(7), int8(1)\n",
      "memory usage: 24.8 MB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(train_data,test_data):\n",
    " \n",
    "    # 合并数据，方便处理和构造特征  \n",
    "    data = pd.concat([train_data,test_data],ignore_index=True)\n",
    "   \n",
    "    # 将索引值生成‘pid_id’列\n",
    "    data.reset_index(inplace=True)\n",
    "    data.rename(columns={'index':'pid_id'},inplace=True)\n",
    "\n",
    "    # 类别特征： 'brand','model','province','city'\n",
    "    brand_dict = {}\n",
    "    brand_list = list(data['brand'].unique())\n",
    "    for ind,val in enumerate(brand_list):\n",
    "        brand_dict[val] = ind\n",
    "    data['brand'] = data['brand'].map(brand_dict).astype('float')\n",
    "    \n",
    "    model_dict = {}\n",
    "    model_list = list(data['model'].unique())\n",
    "    for ind,val in enumerate(model_list):\n",
    "        model_dict[val] = ind\n",
    "    data['model'] = data['model'].map(model_dict).astype('float')\n",
    "    \n",
    "    province_dict = {}\n",
    "    province_list = list(data['province'].unique())\n",
    "    for ind,val in enumerate(province_list):\n",
    "        province_dict[val] = ind\n",
    "    data['province'] = data['province'].map(province_dict).astype('float')\n",
    "    \n",
    "    city_dict = {}\n",
    "    city_list = list(data['city'].unique())\n",
    "    for ind,val in enumerate(city_list):\n",
    "        city_dict[val] = ind\n",
    "    data['city'] = data['city'].map(city_dict).astype('float')\n",
    "    \n",
    "    \n",
    "    # 时间特征 ：‘'enum_tag'，'numeric_tag'\n",
    "    # 将‘unkown’替换成空值\n",
    "    data['enum_tag'] = data['enum_tag'].apply(lambda x:x if x!='unkown' else np.nan)\n",
    "    data['numeric_tag'] = data['numeric_tag'].apply(lambda x:x if x!='unkown' else np.nan)\n",
    "       \n",
    "    # 将 ‘numeric_tag’列的空值用‘enum_tag’的值填充\n",
    "    null_numeric_index = data[data['numeric_tag'].isnull()].index\n",
    "    for i in null_numeric_index:\n",
    "        data['numeric_tag'][i] = data['enum_tag'][i]\n",
    "    del data['enum_tag']\n",
    "    \n",
    "    # 拆分‘numeric_tag’列\n",
    "    # 1）一行拆多行（将父类订单拆成子订单） \n",
    "    data =data.drop('numeric_tag',axis=1).join(data['numeric_tag'].str.split('|',expand=True).stack().reset_index(level=1,drop=True).rename('numeric_tag'))\n",
    "    data['numeric_tag'] =[i.replace(\"'\",'') if \"'\" in str(i) else i for i in data['numeric_tag']]\n",
    "    \n",
    "    # 2） 一列拆多列，将'tag'拆分成'tagid'、'time'列    \n",
    "    data['session_tagid']=data['numeric_tag'].apply(lambda x:str(x).split(';')[0])\n",
    "    data['session_tagid']=data['session_tagid'].apply(lambda x:str(x)[str(x).find(':')+1:])\n",
    "    data['session_time']=data['numeric_tag'].apply(lambda x:str(x)[str(x).find('time'):])\n",
    "    data['session_time']=data['session_time'].apply(lambda x:str(x)[str(x).find(':')+1:])     \n",
    "    data['session_time']= pd.to_datetime(data['session_time'],format='%Y-%m-%d', errors='coerce')\n",
    "    data['value']=data['numeric_tag'].apply(session_value)\n",
    "    data['value'] = data['value'].apply(lambda x:str(x)[str(x).find(':')+1:]) \n",
    "    del data['numeric_tag']\n",
    "\n",
    "    # 压缩数据，通过调整数据类型，减少数据在内存中占用的空间\n",
    "    reduce_list = ['brand','model','province','city','train']\n",
    "    data[reduce_list] = data[reduce_list].astype(np.int16)\n",
    "    data['value'] = data['value'].astype(np.float16)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def session_value(x):\n",
    "    if str(x).find('value')==-1:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return str(x)[str(x).find('value'):str(x).find('time')-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_preprocessing(train_data,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('./feature/feature_0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征工程 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering_1(data):\n",
    "  \n",
    "    # 时间特征提取\n",
    "    data['year'] = data['session_time'].dt.year\n",
    "    data['month'] = data['session_time'].dt.month\n",
    "    data['dayofweek'] = data['session_time'].dt.dayofweek\n",
    "    \n",
    "    # 处理‘year’等于1970的异常数据\n",
    "    data['year'] = data['year'].apply(lambda x:x if x!=1970.0 else np.nan)\n",
    "    data['session_time'] = data['session_time'].apply(lambda x:x if str(x)[:4]!='1970' else np.nan)\n",
    "    \n",
    "    data['first_year'] = data.groupby(by=['pid'])['year'].transform('min')\n",
    "    data['last_year'] = data.groupby(by=['pid'])['year'].transform('max')\n",
    "    \n",
    "    # 分类特征编码 'first_year','last_year'\n",
    "    year_dict = {\n",
    "            2023.0: 0,\n",
    "            2022.0: 1,\n",
    "            2021.0: 2,\n",
    "            2020.0: 3,\n",
    "            2019.0: 4,\n",
    "            2018.0: 5,\n",
    "            2017.0: 6,\n",
    "            2015.0: 7,\n",
    "            2010.0: 8,\n",
    "            2009.0: 9}\n",
    "    data['first_year'] = data['first_year'].map(year_dict).astype('int8')\n",
    "    data['last_year'] = data['last_year'].map(year_dict).astype('int8')\n",
    "    \n",
    "    \n",
    "    # 提取时间特征\n",
    "    data = feature_merge(data,'pid')\n",
    "    \n",
    "    \n",
    "    data['active_days'] = data['active_days'].apply(lambda x:x if x!=0 else -1)\n",
    "    data['dif_days_total'] = (data['last_time'].astype('datetime64') - data['first_time'].astype('datetime64')).dt.days\n",
    "    data['dif_days_avg'] = data['dif_days_total']/data['active_days']\n",
    "    \n",
    "    data['one_day_sessions'] = data.groupby(by=['pid','session_time'])['session_tagid'].transform('count')\n",
    "    data['one_day_sessions_avg'] = data.groupby(by=['pid'])['one_day_sessions'].transform('mean').round(2)\n",
    "    del data['one_day_sessions']\n",
    "    \n",
    "    # 时间特征衍生：‘data_2023'数据集\n",
    "    data_2023 = data[data['year']==2023][['pid_id','session_time']]\n",
    "    data_2023 = feature_merge_2023(data_2023,'pid_id')\n",
    "    \n",
    "    data_2023['2023_active_days'] = data_2023['2023_active_days'].apply(lambda x:x if x!=0 else -1)\n",
    "    data_2023['2023_recency'] = (pd.to_datetime('20230805',format='%Y-%m-%d') - data_2023['2023_last_time']).dt.days\n",
    "    data_2023['2023_dif_days_total'] = (data_2023['2023_last_time'] - data_2023['2023_first_time']).dt.days\n",
    "    data_2023['2023_dif_days_avg'] = (data_2023['2023_dif_days_total']/data_2023['2023_active_days']).round(2)    \n",
    "    data_2023['2023_day_sessions'] = data_2023[data_2023['last_year']==2023].groupby(by=['pid','session_time']).transform('count')\n",
    "    data_2023['2023_day_sessions_avg'] = data_2023.groupby(by=['pid'])['2023_day_sessions'].transform('mean').round(2)\n",
    "    \n",
    "    # 删除一些特征并整体去重\n",
    "    del data_2023['session_time']\n",
    "    del data_2023['2023_day_sessions']\n",
    "    data_2023.drop_duplicates(inplace=True)\n",
    "     \n",
    "    del data['session_tagid']\n",
    "    del data['session_time']\n",
    "    del data['year']\n",
    "    del data['month']\n",
    "    del data['dayofweek']\n",
    "    data.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # data和data_2023拼接\n",
    "    data = data.merge(data_2023,how='left',on='pid_id')\n",
    "        \n",
    "    \n",
    "    # 所有空值用‘-1’填充\n",
    "    data = data.fillna(-1)\n",
    "    \n",
    "    # 数据压缩，通过调整数据类型，减少数据在内存中占用的空间\n",
    "    data = reduce_mem_usage(data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def feature_merge(data,feature):\n",
    "    data_gb = data.groupby(feature)\n",
    "    all_infos = {} \n",
    "    for key,value in data_gb:\n",
    "        info = {}\n",
    "        value = value[value['session_time']!=np.nan]\n",
    "        info['session_count'] = len(value)\n",
    "        info['first_time'] = value.session_time.min()\n",
    "        info['last_time'] = value.session_time.max()\n",
    "        info['active_days'] = value.session_time.nunique()\n",
    "        all_infos[key] = info   \n",
    "    df = pd.DataFrame(all_infos).T.reset_index().rename(columns={'index':str(feature)})\n",
    "    data = data.merge(df,how='left',on=str(feature))    \n",
    "    return data  \n",
    "\n",
    "\n",
    "def feature_merge_2023(data,feature):\n",
    "    data_gb = data.groupby(feature)\n",
    "    all_infos = {} \n",
    "    for key,value in data_gb:\n",
    "        info = {}\n",
    "        value = value[value['session_time']!=np.nan]\n",
    "        info['2023_session_count'] = len(value)\n",
    "        info['2023_first_time'] = value.session_time.min()\n",
    "        info['2023_last_time'] = value.session_time.max()\n",
    "        info['2023_active_days'] = value.session_time.nunique()\n",
    "        all_infos[key] = info   \n",
    "    df = pd.DataFrame(all_infos).T.reset_index().rename(columns={'index':str(feature)})\n",
    "    data = data.merge(df,how='left',on=str(feature))    \n",
    "    return data  \n",
    "\n",
    "\n",
    "def reduce_mem_usage(data):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = data.memory_usage().sum() \n",
    "    \n",
    "    for col in data.columns:\n",
    "        col_type = data[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = data[col].min()\n",
    "            c_max = data[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    data[col] = data[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    data[col] = data[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    data[col] = data[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    data[col] = data[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    data[col] = data[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    data[col] = data[col].astype(np.float32)\n",
    "                else:\n",
    "                    data[col] = data[col].astype(np.float64)\n",
    "        else:\n",
    "            data[col] = data[col].astype('category')\n",
    "    end_mem = data.memory_usage().sum() \n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = feature_engineering_1(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1.to_csv('./feature/feature_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征工程 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering_2(data):\n",
    "  \n",
    "    # 时间特征提取\n",
    "    data['year'] = data['session_time'].dt.year\n",
    "    data['month'] = data['session_time'].dt.month\n",
    "    data['dayofweek'] = data['session_time'].dt.dayofweek\n",
    "    \n",
    "   \n",
    "    # 时间特征衍生‘session_amount’，‘last_time’，'first_time'\n",
    "    data['session_count'] = data.groupby(by=['pid_id'])['session_tagid'].transform('count')\n",
    "    data['first_time'] = data.groupby(by=['pid_id'])['session_time'].transform('min')\n",
    "    data['last_time'] = data.groupby(by=['pid_id'])['session_time'].transform('max')\n",
    "    \n",
    "    data['first_year'] = data.groupby(by=['pid_id'])['year'].transform('min')\n",
    "    data['last_year'] = data.groupby(by=['pid_id'])['year'].transform('max')\n",
    "    \n",
    "    data['dif_days'] = (data['last_time'].astype('datetime64') - data['first_time'].astype('datetime64')).dt.days\n",
    "    data['dif_days_avg'] = data['dif_days']/data['session_amount']\n",
    "        \n",
    "    data['2023_recency'] = (pd.to_datetime('20230805',format='%Y-%m-%d') - data['last_time']).dt.days\n",
    "    data['2023_frequency'] = data[data['year']==2023].groupby('pid_id')['session_time'].transform('count')\n",
    "    data['2023_frequency'] = data['2023_frequency'].apply(lambda x:x if x>-1 else -1)\n",
    "    data['2023_frequency'] = data.groupby('pid_id')['2023_frequency'].transform('max')    \n",
    "    \n",
    "    data['2023_last_time'] = data[data['year']==2023].groupby('pid_id')['session_time'].transform('max')\n",
    "    data['2023_last_time'] = data.groupby('pid_id')['2023_last_time'].transform('max')\n",
    "    data['2023_first_time'] = data[data['year']==2023].groupby('pid_id')['session_time'].transform('min')\n",
    "    data['2023_first_time'] = data.groupby('pid_id')['2023_first_time'].transform('min')\n",
    "    \n",
    "    data['2023_dif_days'] = (data['2023_last_time'].astype('datetime64') - data['2023_first_time'].astype('datetime64')).dt.days\n",
    "    data['2023_dif_days_avg'] = data['2023_dif_days']/data['2023_frequency']\n",
    "    \n",
    "    del data['session_tagid']\n",
    "    del data['session_time']\n",
    "    del data['last_time']\n",
    "    del data['first_time']\n",
    "    del data['2023_last_time']\n",
    "    del data['2023_first_time']\n",
    "    \n",
    "    \n",
    "    # 分类特征编码\n",
    "    class_cols = ['first_year','last_year']\n",
    "    for col in class_cols:\n",
    "        lbl = LabelEncoder().fit(data[col])\n",
    "        data[col] = lbl.transform(data[col])  \n",
    "    \n",
    "    # 时间统计特征独热 'year','month','dayofweek'\n",
    "    onehot_list = ['pid_id','year','month','dayofweek']\n",
    "    onehot_data = data[onehot_list]\n",
    "    onehot_data = pd.get_dummies(onehot_data,columns=['year','month','dayofweek'])\n",
    "    feature_list = onehot_data.drop('pid_id',axis=1).columns\n",
    "    for feat in feature_list:\n",
    "        onehot_data[str(feat) +'_count'] = onehot_data.groupby('pid_id')[feat].transform('sum')\n",
    "    drop_list = ['year_1970.0', 'year_2009.0', 'year_2010.0', 'year_2015.0',\n",
    "       'year_2017.0', 'year_2018.0', 'year_2019.0', 'year_2020.0',\n",
    "       'year_2021.0', 'year_2022.0', 'year_2023.0', 'month_1.0', 'month_2.0',\n",
    "       'month_3.0', 'month_4.0', 'month_5.0', 'month_6.0', 'month_7.0',\n",
    "       'month_8.0', 'month_9.0', 'month_10.0', 'month_11.0', 'month_12.0',\n",
    "       'dayofweek_0.0', 'dayofweek_1.0', 'dayofweek_2.0', 'dayofweek_3.0',\n",
    "       'dayofweek_4.0', 'dayofweek_5.0', 'dayofweek_6.0']\n",
    "    onehot_data = onehot_data.drop(columns=drop_list,axis=1)\n",
    "    onehot_data.drop_duplicates(inplace=True)\n",
    "\n",
    "    \n",
    "    # 删除一些特征并整体去重\n",
    "    del data['pid'] \n",
    "    del data['year']\n",
    "    del data['month']\n",
    "    del data['dayofweek']\n",
    "    data.drop_duplicates(inplace=True)\n",
    "\n",
    "    # data和onehot_data拼接   \n",
    "    data = data.merge(onehot_data,how='left',on='pid_id')\n",
    "\n",
    "    \n",
    "    # 所有空值用‘-1’填充\n",
    "    data = data.fillna(-1)\n",
    "    \n",
    "    #删除‘pid_id’\n",
    "    del data['pid_id']\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 数据压缩，通过调整数据类型，减少数据在内存中占用的空间\n",
    "    data = reduce_mem_usage(data)\n",
    "        \n",
    "    return data\n",
    "\n",
    "\n",
    "def reduce_mem_usage(data):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = data.memory_usage().sum() \n",
    "    \n",
    "    for col in data.columns:\n",
    "        col_type = data[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = data[col].min()\n",
    "            c_max = data[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    data[col] = data[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    data[col] = data[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    data[col] = data[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    data[col] = data[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    data[col] = data[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    data[col] = data[col].astype(np.float32)\n",
    "                else:\n",
    "                    data[col] = data[col].astype(np.float64)\n",
    "        else:\n",
    "            data[col] = data[col].astype('category')\n",
    "    end_mem = data.memory_usage().sum() \n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2 = feature_engineering_2(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2.to_csv('./feature/feature_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征工程 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering_3(data):\n",
    "  \n",
    "    # 时间特征提取\n",
    "    data['year'] = data['session_time'].dt.year\n",
    "    data['month'] = data['session_time'].dt.month\n",
    "    data['dayofweek'] = data['session_time'].dt.dayofweek\n",
    "    \n",
    "    # 处理‘year’等于1970的异常数据\n",
    "    data['year'] = data['year'].apply(lambda x:x if x!=1970.0 else np.nan)\n",
    "    data['session_time'] = data['session_time'].apply(lambda x:x if str(x)[:4]!='1970' else np.nan)\n",
    "    \n",
    "    data['first_year'] = data.groupby(by=['pid'])['year'].transform('min')\n",
    "    data['last_year'] = data.groupby(by=['pid'])['year'].transform('max')    \n",
    "    \n",
    "    # 分类特征编码 'first_year','last_year'\n",
    "    year_dict = {\n",
    "            -1.0: -1,\n",
    "            2023.0: 0,\n",
    "            2022.0: 1,\n",
    "            2021.0: 2,\n",
    "            2020.0: 3,\n",
    "            2019.0: 4,\n",
    "            2018.0: 5,\n",
    "            2017.0: 6,\n",
    "            2015.0: 7,\n",
    "            2010.0: 8,\n",
    "            2009.0: 9}\n",
    "    data['first_year'] = data['first_year'].map(year_dict).astype('int8')\n",
    "    data['last_year'] = data['last_year'].map(year_dict).astype('int8')\n",
    "    \n",
    "    \n",
    "    # 提取时间特征\n",
    "    data = feature_merge(data,'pid')\n",
    "    data['active_days'] = data['active_days'].apply(lambda x:x if x!=0 else -1)\n",
    "    data['dif_days_total'] = (data['last_time'].astype('datetime64') - data['first_time'].astype('datetime64')).dt.days\n",
    "    data['dif_days_avg'] = data['dif_days_total']/data['active_days']\n",
    "    \n",
    "    data['one_day_sessions'] = data.groupby(by=['pid','session_time'])['session_tagid'].transform('count')\n",
    "    data['one_day_sessions_avg'] = data.groupby(by=['pid'])['one_day_sessions'].transform('mean').round(2)\n",
    "    del data['one_day_sessions']\n",
    "    \n",
    "    # 时间特征衍生：‘data_2023'数据集\n",
    "    data_2023 = data[data['year']==2023][['pid_id','session_time']]\n",
    "    data_2023 = feature_merge_2023(data_2023,'pid_id')\n",
    "    \n",
    "    data_2023['2023_active_days'] = data_2023['2023_active_days'].apply(lambda x:x if x!=0 else -1)\n",
    "    data_2023['2023_recency'] = (pd.to_datetime('20230805',format='%Y-%m-%d') - data_2023['2023_last_time']).dt.days\n",
    "    data_2023['2023_dif_days_total'] = (data_2023['2023_last_time'] - data_2023['2023_first_time']).dt.days\n",
    "    data_2023['2023_dif_days_avg'] = (data_2023['2023_dif_days_total']/data_2023['2023_active_days']).round(2)    \n",
    "    data_2023['2023_day_sessions'] = data_2023[data_2023['last_year']==2023].groupby(by=['pid','session_time']).transform('count')\n",
    "    data_2023['2023_day_sessions_avg'] = data_2023.groupby(by=['pid'])['2023_day_sessions'].transform('mean').round(2)\n",
    "    \n",
    "    # data_2023 删除一些特征并整体去重\n",
    "    del data_2023['session_time']\n",
    "    del data_2023['2023_day_sessions']\n",
    "    data_2023.drop_duplicates(inplace=True)\n",
    "          \n",
    "    \n",
    "    # 时间统计特征独热 :onehot_data('year','month','dayofweek')\n",
    "    onehot_list = ['pid_id','year','month','dayofweek']\n",
    "    onehot_data = data[onehot_list]\n",
    "    onehot_data = pd.get_dummies(onehot_data,columns=['year','month','dayofweek'])\n",
    "    feature_list = onehot_data.drop('pid_id',axis=1).columns\n",
    "    for feat in feature_list:\n",
    "        onehot_data[str(feat) +'_count'] = onehot_data.groupby('pid_id')[feat].transform('sum')\n",
    "    drop_list = [ 'year_2009.0', 'year_2010.0', 'year_2015.0',\n",
    "       'year_2017.0', 'year_2018.0', 'year_2019.0', 'year_2020.0',\n",
    "       'year_2021.0', 'year_2022.0', 'year_2023.0', 'month_1.0', 'month_2.0',\n",
    "       'month_3.0', 'month_4.0', 'month_5.0', 'month_6.0', 'month_7.0',\n",
    "       'month_8.0', 'month_9.0', 'month_10.0', 'month_11.0', 'month_12.0',\n",
    "       'dayofweek_0.0', 'dayofweek_1.0', 'dayofweek_2.0', 'dayofweek_3.0',\n",
    "       'dayofweek_4.0', 'dayofweek_5.0', 'dayofweek_6.0']\n",
    "    onehot_data = onehot_data.drop(columns=drop_list,axis=1)\n",
    "    onehot_data.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # 删除一些特征，整体去重\n",
    "    del data['pid']\n",
    "    del data['label']\n",
    "    del data['session_tagid']\n",
    "    del data['session_time']\n",
    "    del data['year']\n",
    "    del data['month']\n",
    "    del data['dayofweek']    \n",
    "    data.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # data和data_2023拼接\n",
    "    data = data.merge(data_2023,how='left',on='pid_id')\n",
    "    \n",
    "    # data和onehot_data拼接\n",
    "    data = data.merge(onehot_data,how='left',on='pid_id')\n",
    "\n",
    "    \n",
    "    # 所有空值用‘-1’填充\n",
    "    data = data.fillna(-1)\n",
    "    \n",
    "    # 数据压缩，通过调整数据类型，减少数据在内存中占用的空间\n",
    "    data = reduce_mem_usage(data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def feature_merge(data,feature):\n",
    "    data_gb = data.groupby(feature)\n",
    "    all_infos = {} \n",
    "    for key,value in data_gb:\n",
    "        info = {}\n",
    "        value = value[value['session_time']!=np.nan]\n",
    "        info['session_count'] = len(value)\n",
    "        info['first_time'] = value.session_time.min()\n",
    "        info['last_time'] = value.session_time.max()\n",
    "        info['active_days'] = value.session_time.nunique()\n",
    "        all_infos[key] = info   \n",
    "    df = pd.DataFrame(all_infos).T.reset_index().rename(columns={'index':str(feature)})\n",
    "    data = data.merge(df,how='left',on=str(feature))    \n",
    "    return data  \n",
    "\n",
    "def feature_merge_2023(data,feature):\n",
    "    data_gb = data.groupby(feature)\n",
    "    all_infos = {} \n",
    "    for key,value in data_gb:\n",
    "        info = {}\n",
    "        value = value[value['session_time']!=np.nan]\n",
    "        info['2023_session_count'] = len(value)\n",
    "        info['2023_first_time'] = value.session_time.min()\n",
    "        info['2023_last_time'] = value.session_time.max()\n",
    "        info['2023_active_days'] = value.session_time.nunique()\n",
    "        all_infos[key] = info   \n",
    "    df = pd.DataFrame(all_infos).T.reset_index().rename(columns={'index':str(feature)})\n",
    "    data = data.merge(df,how='left',on=str(feature))    \n",
    "    return data  \n",
    "\n",
    "\n",
    "def is_year_2023(x):\n",
    "    if x==2023.0:\n",
    "        return 1\n",
    "    elif x==-1:\n",
    "        return x\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    \n",
    "def reduce_mem_usage(data):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = data.memory_usage().sum() \n",
    "    \n",
    "    for col in data.columns:\n",
    "        col_type = data[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = data[col].min()\n",
    "            c_max = data[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    data[col] = data[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    data[col] = data[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    data[col] = data[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    data[col] = data[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    data[col] = data[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    data[col] = data[col].astype(np.float32)\n",
    "                else:\n",
    "                    data[col] = data[col].astype(np.float64)\n",
    "        else:\n",
    "            data[col] = data[col].astype('category')\n",
    "    end_mem = data.memory_usage().sum() \n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_3 = feature_engineering_3(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_3.to_csv('./feature/feature_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征工程 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering_4(data):\n",
    "  \n",
    "    # 时间特征提取\n",
    "    data['year'] = data['session_time'].dt.year\n",
    "    data['month'] = data['session_time'].dt.month\n",
    "    data['dayofweek'] = data['session_time'].dt.dayofweek\n",
    "    \n",
    "    # 处理‘year’等于1970的异常数据\n",
    "    data['year'] = data['year'].apply(lambda x:x if x!=1970.0 else np.nan)\n",
    "    data['session_time'] = data['session_time'].apply(lambda x:x if str(x)[:4]!='1970' else np.nan)\n",
    "    \n",
    "    # 时间特征衍生\n",
    "    data['first_year'] = data.groupby(by=['pid'])['year'].transform('min')\n",
    "    data['last_year'] = data.groupby(by=['pid'])['year'].transform('max')\n",
    "    data['is_first_year_2023'] = data['first_year'].apply(is_year_2023)\n",
    "    data['is_last_year_2023'] = data['last_year'].apply(is_year_2023)\n",
    "    \n",
    "    data['session_count'] = data.groupby(by=['pid'])['session_tagid'].transform('count')\n",
    "    data['first_time'] = data.groupby(by=['pid'])['session_time'].transform('min')\n",
    "    data['last_time'] = data.groupby(by=['pid'])['session_time'].transform('max')\n",
    "        \n",
    "    data['dif_days_total'] = (data['last_time'].astype('datetime64') - data['first_time'].astype('datetime64')).dt.days\n",
    "    data['dif_days_avg'] = (data['dif_days_total']/data['session_count']).round(2)\n",
    "    \n",
    "    data['active_days'] = data.groupby(by=['pid'])['session_time'].transform('count')\n",
    "    data['one_day_sessions'] = data.groupby(by=['pid','session_time'])['session_tagid'].transform('count')\n",
    "    data['one_day_sessions_avg'] = data.groupby(by=['pid'])['one_day_sessions'].transform('mean').round(2)\n",
    "    \n",
    "    data['2023_recency'] = (pd.to_datetime('20230805',format='%Y-%m-%d') - data['last_time']).dt.days\n",
    "    data['2023_session_count'] = data[data['last_year']==2023].groupby('pid')['session_tagid'].transform('count')\n",
    "    data['2023_last_time'] = data[data['last_year']==2023].groupby('pid')['session_time'].transform('max')\n",
    "    data['2023_first_time'] = data[data['last_year']==2023].groupby('pid')['session_time'].transform('min')\n",
    "    data['2023_dif_days_total'] = (data['2023_last_time'] - data['2023_first_time']).dt.days\n",
    "    data['2023_dif_days_avg'] = (data['2023_dif_days_total']/data['2023_session_count']).round(2)     \n",
    "    data['2023_active_days'] = data[data['last_year']==2023].groupby(by=['pid'])['session_time'].transform('count')\n",
    "    data['2023_day_sessions'] = data[data['last_year']==2023].groupby(by=['pid','session_time'])['session_tagid'].transform('count')\n",
    "    data['2023_day_sessions_avg'] = data.groupby(by=['pid'])['2023_day_sessions'].transform('mean').round(2)\n",
    "    \n",
    "    # 分类特征编码 'first_year','last_year'\n",
    "    year_dict = {\n",
    "            -1.0: -1,\n",
    "            2023.0: 0,\n",
    "            2022.0: 1,\n",
    "            2021.0: 2,\n",
    "            2020.0: 3,\n",
    "            2019.0: 4,\n",
    "            2018.0: 5,\n",
    "            2017.0: 6,\n",
    "            2015.0: 7,\n",
    "            2010.0: 8,\n",
    "            2009.0: 9}\n",
    "    data['first_year'] = data['first_year'].map(year_dict).astype('int8')\n",
    "    data['last_year'] = data['last_year'].map(year_dict).astype('int8')\n",
    "    \n",
    "    \n",
    "    del data['pid'] \n",
    "    del data['session_tagid']\n",
    "    del data['session_time']\n",
    "    del data['last_time']\n",
    "    del data['first_time']\n",
    "    del data['2023_last_time']\n",
    "    del data['2023_first_time']\n",
    "    del data['one_day_sessions']\n",
    "    del data['2023_day_sessions']\n",
    "    \n",
    "    # 时间统计特征独热 'year','month','dayofweek'\n",
    "    onehot_list = ['pid_id','year','month','dayofweek']\n",
    "    onehot_data = data[onehot_list]\n",
    "    onehot_data = pd.get_dummies(onehot_data,columns=['year','month','dayofweek'])\n",
    "    feature_list = onehot_data.drop('pid_id',axis=1).columns\n",
    "    for feat in feature_list:\n",
    "        onehot_data[str(feat) +'_count'] = onehot_data.groupby('pid_id')[feat].transform('sum')\n",
    "    drop_list = [ 'year_2009.0', 'year_2010.0', 'year_2015.0',\n",
    "       'year_2017.0', 'year_2018.0', 'year_2019.0', 'year_2020.0',\n",
    "       'year_2021.0', 'year_2022.0', 'year_2023.0', 'month_1.0', 'month_2.0',\n",
    "       'month_3.0', 'month_4.0', 'month_5.0', 'month_6.0', 'month_7.0',\n",
    "       'month_8.0', 'month_9.0', 'month_10.0', 'month_11.0', 'month_12.0',\n",
    "       'dayofweek_0.0', 'dayofweek_1.0', 'dayofweek_2.0', 'dayofweek_3.0',\n",
    "       'dayofweek_4.0', 'dayofweek_5.0', 'dayofweek_6.0']\n",
    "    onehot_data = onehot_data.drop(columns=drop_list,axis=1)\n",
    "    onehot_data.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # 删除一些特征，整体去重\n",
    "    del data['year']\n",
    "    del data['month']\n",
    "    del data['dayofweek']\n",
    "    \n",
    "    data.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # data和onehot_data拼接\n",
    "    data = data.merge(onehot_data,how='left',on='pid_id')\n",
    "    \n",
    "    \n",
    "    # 处理数值特征的异常数据，并进行数据分桶\n",
    "    bins_c = [0,1,3,5,10,24,35,48,56,72,95,500]\n",
    "    data['session_count_bin'] = pd.cut(data['session_count'],bins_c,labels=False)\n",
    "    \n",
    "    bins_act = [-1,1,2,3,7,10,15,20,32,90]\n",
    "    data['active_days_bin'] = pd.cut(data['active_days'],bins_act,labels=False)\n",
    "    \n",
    "    bins_dif = [-1,0,7,14,30,60,90,120,150,180,240,365,730,1095,20000]\n",
    "    data['dif_days_bin'] = pd.cut(data['dif_days_total'],bins_dif,labels=False)\n",
    "    \n",
    "    bins_dif_avg = [-1,0,7,14,21,30,60,90,120,180,365,10000]\n",
    "    data['dif_days_avg_bin'] = pd.cut(data['dif_days_avg'],bins_dif_avg,labels=False)\n",
    "    \n",
    "    bins_se = [-1,1,2,3,5,12,24,35,65,500]\n",
    "    data['day_sessions_avg_bin'] = pd.cut(data['day_sessions_avg'],bins_se,labels=False)\n",
    "    \n",
    "    bins_rec = [-1,60,90,120,150,180,220]\n",
    "    data['2023_recency_bin'] = pd.cut(data['2023_recency'],bins_rec,labels=False)\n",
    "    \n",
    "    bins_sec = [0,1,3,5,10,21,27,32,43,65,120,320]\n",
    "    data['2023_session_count_bin'] = pd.cut(data['2023_session_count'],bins_sec,labels=False)\n",
    "    \n",
    "    bins_acd = [0,1,4,9,12,20,25]\n",
    "    data['2023_active_days_bin'] = pd.cut(data['2023_active_days'],bins_acd,labels=False)\n",
    "    \n",
    "    bins_2023_dif = [-1,0,3,7,14,21,30,60,90,120,180,200]\n",
    "    data['2023_dif_days_bin'] = pd.cut(data['2023_dif_days_total'],bins_2023_dif,labels=False)\n",
    "    \n",
    "    bins_2023_dda= [-1,0,3,7,15,23,31,40,63,100]\n",
    "    data['2023_dif_days_avg_bin'] = pd.cut(data['2023_dif_days_avg'],bins_dda,labels=False)\n",
    "    \n",
    "    bins_dse = [0,1,3,5,9,15,300]\n",
    "    data['2023_day_sessions_avg_bin'] = pd.cut(data['2023_day_sessions_avg'],bins_dse,labels=False)  \n",
    "    \n",
    "    \n",
    "    # 所有空值用‘-1’填充\n",
    "    data = data.fillna(-1)\n",
    "    \n",
    "    \n",
    "    # 数据压缩，通过调整数据类型，减少数据在内存中占用的空间\n",
    "    data = reduce_mem_usage(data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def is_year_2023(x):\n",
    "    if x==2023.0:\n",
    "        return 1\n",
    "    elif x==-1:\n",
    "        return x\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def reduce_mem_usage(data):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = data.memory_usage().sum() \n",
    "    \n",
    "    for col in data.columns:\n",
    "        col_type = data[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = data[col].min()\n",
    "            c_max = data[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    data[col] = data[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    data[col] = data[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    data[col] = data[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    data[col] = data[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    data[col] = data[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    data[col] = data[col].astype(np.float32)\n",
    "                else:\n",
    "                    data[col] = data[col].astype(np.float64)\n",
    "        else:\n",
    "            data[col] = data[col].astype('category')\n",
    "    end_mem = data.memory_usage().sum() \n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征最终完整版"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_feature(train_data,test_data):\n",
    "    \n",
    "    # 合并数据，方便处理和构造特征  \n",
    "    data = pd.concat([train_data,test_data],ignore_index=True)\n",
    "\n",
    "    # 将索引值生成‘pid_id’列\n",
    "    data.reset_index(inplace=True)\n",
    "    data.rename(columns={'index':'pid_id'},inplace=True)\n",
    "    \n",
    "    # 数据预处理\n",
    "    data = data_preprocessing(data)\n",
    "    \n",
    "    # 特征工程\n",
    "    data = feature_engineering(data)\n",
    "         \n",
    "    # 筛选特征\n",
    "#     data = select_feature(data)    \n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def data_preprocessing(data):\n",
    "   \n",
    "    # 时间特征 ：‘'enum_tag'，'numeric_tag'\n",
    "    # 将‘unkown’替换成空值\n",
    "    data['enum_tag'] = data['enum_tag'].apply(lambda x:x if x!='unkown' else np.nan)\n",
    "    data['numeric_tag'] = data['numeric_tag'].apply(lambda x:x if x!='unkown' else np.nan)\n",
    "       \n",
    "    # 将 ‘numeric_tag’列的空值用‘enum_tag’的值填充\n",
    "    null_numeric_index = data[data['numeric_tag'].isnull()].index\n",
    "    for i in null_numeric_index:\n",
    "        data['numeric_tag'][i] = data['enum_tag'][i]\n",
    "    del data['enum_tag']\n",
    "    \n",
    "    # 拆分‘numeric_tag’列\n",
    "    # 1）一行拆多行（将父类订单拆成子订单） \n",
    "    data =data.drop('numeric_tag',axis=1).join(data['numeric_tag'].str.split('|',expand=True).stack().reset_index(level=1,drop=True).rename('numeric_tag'))\n",
    "    data['numeric_tag'] =[i.replace(\"'\",'') if \"'\" in str(i) else i for i in data['numeric_tag']]\n",
    "    \n",
    "    # 2） 一列拆多列，将'tag'拆分成'tagid'、'time'列    \n",
    "    data['session_tagid']=data['numeric_tag'].apply(lambda x:str(x).split(';')[0])\n",
    "    data['session_tagid']=data['session_tagid'].apply(lambda x:str(x)[str(x).find(':')+1:])\n",
    "    data['session_time']=data['numeric_tag'].apply(lambda x:str(x)[str(x).find('time'):])\n",
    "    data['session_time']=data['session_time'].apply(lambda x:str(x)[str(x).find(':')+1:])     \n",
    "    data['session_time']= pd.to_datetime(data['session_time'],format='%Y-%m-%d', errors='coerce')\n",
    "    data['value']=data['numeric_tag'].apply(session_value)\n",
    "    data['value'] = data['value'].apply(lambda x:str(x)[str(x).find(':')+1:]) \n",
    "    del data['numeric_tag']\n",
    "\n",
    "    # 压缩数据，通过调整数据类型，减少数据在内存中占用的空间\n",
    "    reduce_list = ['brand','model','province','city']\n",
    "    data[reduce_list] = data[reduce_list].astype(np.int16)\n",
    "    data['value'] = data['value'].astype(np.float16)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def feature_engineering(data):\n",
    "  \n",
    "    # 时间特征提取\n",
    "    data['year'] = data['session_time'].dt.year\n",
    "    data['month'] = data['session_time'].dt.month\n",
    "    data['dayofweek'] = data['session_time'].dt.dayofweek\n",
    "    \n",
    "    # 处理‘year’等于1970的异常数据\n",
    "    data['year'] = data['year'].apply(lambda x:x if x!=1970.0 else np.nan)\n",
    "    data['session_time'] = data['session_time'].apply(lambda x:x if str(x)[:4]!='1970' else np.nan)\n",
    "    \n",
    "    # 'value' 取平均值\n",
    "    data['value_mean'] = data.groupby(by=['pid'])['value'].transform('mean')\n",
    "    del data['value']\n",
    "    \n",
    "    # 时间特征衍生\n",
    "    data['first_year'] = data.groupby(by=['pid'])['year'].transform('min')\n",
    "    data['last_year'] = data.groupby(by=['pid'])['year'].transform('max')\n",
    "    \n",
    "#     data['is_first_year_2023'] = data['first_year'].apply(is_year_2023)\n",
    "#     data['is_last_year_2023'] = data['last_year'].apply(is_year_2023)\n",
    "    \n",
    "    # 提取时间特征 ：session_amount’，‘last_time’，'first_time'，'active_days'\n",
    "    data = feature_merge(data,'pid')\n",
    "    \n",
    "    data['dif_days_total'] = (data['last_time'].astype('datetime64') - data['first_time'].astype('datetime64')).dt.days\n",
    "    data['dif_days_avg'] = data['dif_days_total']/data['active_days']\n",
    "    \n",
    "    data['one_day_sessions'] = data.groupby(by=['pid','session_time'])['session_tagid'].transform('count')\n",
    "    data['one_day_sessions_avg'] = data.groupby(by=['pid'])['one_day_sessions'].transform('mean').round(2)\n",
    "    del data['one_day_sessions']\n",
    "    \n",
    "    # 时间特征衍生：‘data_2023'数据集，'2023_session_count'，'2023_last_time’，'2023_first_time'，'2023_active_days'\n",
    "    data_2023 = data[data['year']==2023][['pid_id','session_time']]\n",
    "    data_2023 = feature_merge_2023(data_2023,'pid_id')\n",
    "    \n",
    "    data_2023['2023_recency'] = (pd.to_datetime('20230805',format='%Y-%m-%d') - data_2023['2023_last_time']).dt.days\n",
    "    data_2023['2023_dif_days_total'] = (data_2023['2023_last_time'] - data_2023['2023_first_time']).dt.days\n",
    "    data_2023['2023_dif_days_avg'] = (data_2023['2023_dif_days_total']/data_2023['2023_active_days']).round(2)      \n",
    "    data_2023['2023_day_sessions'] = data_2023.groupby(by=['pid_id','session_time']).transform('count')\n",
    "    data_2023['2023_day_sessions_avg'] = data_2023.groupby(by=['pid_id'])['2023_day_sessions'].transform('mean').round(2)\n",
    "    \n",
    "    # data_2023 删除一些特征并整体去重\n",
    "    del data_2023['session_time']\n",
    "    del data_2023['2023_day_sessions']\n",
    "    data_2023.drop_duplicates(inplace=True)\n",
    "    \n",
    "\n",
    "    \n",
    "    # 时间统计特征独热 'year','month','dayofweek'\n",
    "    onehot_list = ['pid_id','year','month','dayofweek']\n",
    "    onehot_data = data[onehot_list]\n",
    "    onehot_data = pd.get_dummies(onehot_data,columns=['year','month','dayofweek'])\n",
    "    feature_list = onehot_data.drop('pid_id',axis=1).columns\n",
    "    for feat in feature_list:\n",
    "        onehot_data[str(feat) +'_count'] = onehot_data.groupby('pid_id')[feat].transform('sum')\n",
    "    drop_list = [ 'year_2009.0', 'year_2010.0', 'year_2015.0',\n",
    "       'year_2017.0', 'year_2018.0', 'year_2019.0', 'year_2020.0',\n",
    "       'year_2021.0', 'year_2022.0', 'year_2023.0', 'month_1.0', 'month_2.0',\n",
    "       'month_3.0', 'month_4.0', 'month_5.0', 'month_6.0', 'month_7.0',\n",
    "       'month_8.0', 'month_9.0', 'month_10.0', 'month_11.0', 'month_12.0',\n",
    "       'dayofweek_0.0', 'dayofweek_1.0', 'dayofweek_2.0', 'dayofweek_3.0',\n",
    "       'dayofweek_4.0', 'dayofweek_5.0', 'dayofweek_6.0']\n",
    "    onehot_data = onehot_data.drop(columns=drop_list,axis=1)\n",
    "    onehot_data.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # 删除一些特征，整体去重\n",
    "    del data['pid'] \n",
    "    del data['session_tagid']\n",
    "    del data['session_time']\n",
    "    del data['last_time']\n",
    "    del data['first_time']\n",
    "    del data['2023_last_time']\n",
    "    del data['2023_first_time']\n",
    "    del data['one_day_sessions']\n",
    "    del data['2023_day_sessions']\n",
    "    del data['year']\n",
    "    del data['month']\n",
    "    del data['dayofweek'] \n",
    "    data.drop_duplicates(inplace=True)\n",
    "    \n",
    "    \n",
    "    # data和data_2023拼接\n",
    "    data = data.merge(data_2023,how='left',on='pid_id')\n",
    "    \n",
    "    # data和onehot_data拼接\n",
    "    data = data.merge(onehot_data,how='left',on='pid_id')\n",
    "    \n",
    "    \n",
    "    # 类别特征： 'brand','model','province','city'\n",
    "    brand_dict = {}\n",
    "    brand_list = list(data['brand'].unique())\n",
    "    for ind,val in enumerate(brand_list):\n",
    "        brand_dict[val] = ind\n",
    "    data['brand'] = data['brand'].map(brand_dict).astype('float')\n",
    "    \n",
    "    model_dict = {}\n",
    "    model_list = list(data['model'].unique())\n",
    "    for ind,val in enumerate(model_list):\n",
    "        model_dict[val] = ind\n",
    "    data['model'] = data['model'].map(model_dict).astype('float')\n",
    "    \n",
    "    province_dict = {}\n",
    "    province_list = list(data['province'].unique())\n",
    "    for ind,val in enumerate(province_list):\n",
    "        province_dict[val] = ind\n",
    "    data['province'] = data['province'].map(province_dict).astype('float')\n",
    "    \n",
    "    city_dict = {}\n",
    "    city_list = list(data['city'].unique())\n",
    "    for ind,val in enumerate(city_list):\n",
    "        city_dict[val] = ind\n",
    "    data['city'] = data['city'].map(city_dict).astype('float')\n",
    "       \n",
    "    \n",
    "    # 分类特征编码 'first_year','last_year'\n",
    "    year_dict = {\n",
    "            2023.0: 0,\n",
    "            2022.0: 1,\n",
    "            2021.0: 2,\n",
    "            2020.0: 3,\n",
    "            2019.0: 4,\n",
    "            2018.0: 5,\n",
    "            2017.0: 6,\n",
    "            2015.0: 7,\n",
    "            2010.0: 8,\n",
    "            2009.0: 9}\n",
    "    data['first_year'] = data['first_year'].map(year_dict).astype('int8')\n",
    "    data['last_year'] = data['last_year'].map(year_dict).astype('int8')\n",
    " \n",
    "    \n",
    "    # 处理数值特征的异常数据\n",
    "    data['session_count'] = data['session_count'].apply(lambda x:x if x<96 else 100)\n",
    "    data['active_days'] = data['active_days'].apply(lambda x:x if x<32 else 35)\n",
    "    data['dif_days_total'] = data['dif_days_total'].apply(lambda x:x if x<1000 else 1000)\n",
    "    data['dif_days_avg'] = data['dif_days_avg'].apply(lambda x:x if x<140 else 140)\n",
    "    data['day_sessions_avg'] = data['day_sessions_avg'].apply(lambda x:x if x<20 else 20)\n",
    "    data['2023_session_count'] = data['session_count'].apply(lambda x:x if x<50 else 50)\n",
    "    data['2023_day_sessions_avg'] = data['2023_day_sessions_avg'].apply(lambda x:x if x<20 else 20)\n",
    "    \n",
    "    # 衍生新的数值特征\n",
    "    data['2023_session_rate'] = (data['2023_session_count']/data['session_count']).round(2)\n",
    "    data['2023_active_rate'] = (data['2023_active_days']/data['active_days']).round(2)\n",
    "    data['2023_difdays_reduce'] = (data['dif_days_avg'] - data['2023_dif_days_avg']).round(2)\n",
    "    data['2023_day_sessions_pro'] = (data['2023_day_sessions_avg'] - data['day_sessions_avg']).round(2)\n",
    "    \n",
    "    \n",
    "    # 数值特征进行数据分桶\n",
    "    bins_c = [0,1,3,5,10,24,35,48,56,72,95,500]\n",
    "    data['session_count_bin'] = pd.cut(data['session_count'],bins_c,labels=False)\n",
    "    \n",
    "    bins_act = [-1,1,2,3,7,10,15,20,32,90]\n",
    "    data['active_days_bin'] = pd.cut(data['active_days'],bins_act,labels=False)\n",
    "    \n",
    "    bins_dif = [-1,0,7,14,30,60,90,120,150,180,240,365,730,1095,20000]\n",
    "    data['dif_days_bin'] = pd.cut(data['dif_days_total'],bins_dif,labels=False)\n",
    "    \n",
    "    bins_dif_avg = [-1,0,7,14,21,30,60,90,120,180,365,10000]\n",
    "    data['dif_days_avg_bin'] = pd.cut(data['dif_days_avg'],bins_dif_avg,labels=False)\n",
    "    \n",
    "    bins_se = [-1,1,2,3,5,12,24,35,65,500]\n",
    "    data['day_sessions_avg_bin'] = pd.cut(data['day_sessions_avg'],bins_se,labels=False)\n",
    "    \n",
    "    bins_rec = [-1,60,90,120,150,180,220]\n",
    "    data['2023_recency_bin'] = pd.cut(data['2023_recency'],bins_rec,labels=False)\n",
    "    \n",
    "    bins_sec = [0,1,3,5,10,21,27,32,43,65,120,320]\n",
    "    data['2023_session_count_bin'] = pd.cut(data['2023_session_count'],bins_sec,labels=False)\n",
    "    \n",
    "    bins_acd = [0,1,4,9,12,20,25]\n",
    "    data['2023_active_days_bin'] = pd.cut(data['2023_active_days'],bins_acd,labels=False)\n",
    "    \n",
    "    bins_2023_dif = [-1,0,3,7,14,21,30,60,90,120,180,200]\n",
    "    data['2023_dif_days_bin'] = pd.cut(data['2023_dif_days_total'],bins_2023_dif,labels=False)\n",
    "    \n",
    "    bins_2023_dda= [-1,0,3,7,15,23,31,40,63,100]\n",
    "    data['2023_dif_days_avg_bin'] = pd.cut(data['2023_dif_days_avg'],bins_dda,labels=False)\n",
    "    \n",
    "    bins_dse = [0,1,3,5,9,15,300]\n",
    "    data['2023_day_sessions_avg_bin'] = pd.cut(data['2023_day_sessions_avg'],bins_dse,labels=False)  \n",
    "    \n",
    "    \n",
    "    # 空值填充\n",
    "    # 用众数填充\n",
    "    year_col = ['first_year','last_year']\n",
    "    si = SimpleImputer(strategy='most_frequent').fit(data[year_col])\n",
    "    data[year_col] = si.transform(data[year_col])\n",
    "    \n",
    "    # 用 中位数填充\n",
    "    num_col = ['session_count', 'active_days', 'dif_days_total',\n",
    "       'dif_days_avg', 'day_sessions_avg', '2023_session_count',\n",
    "       '2023_active_days', '2023_recency', '2023_dif_days_total',\n",
    "       '2023_dif_days_avg', '2023_day_sessions_avg','value_mean']\n",
    "    si = SimpleImputer(strategy='median').fit(data[num_col])\n",
    "    data[num_col] = si.transform(data[num_col])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 数据压缩，通过调整数据类型，减少数据在内存中占用的空间\n",
    "    data = reduce_mem_usage(data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def session_value(x):\n",
    "    if str(x).find('value')==-1:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return str(x)[str(x).find('value'):str(x).find('time')-1]\n",
    "    \n",
    "# def is_year_2023(x):\n",
    "#     if x==2023.0:\n",
    "#         return 1\n",
    "#     elif x==-1:\n",
    "#         return x\n",
    "#     else:\n",
    "#         return 0\n",
    "\n",
    "def feature_merge(data,feature):\n",
    "    data_gb = data.groupby(feature)\n",
    "    all_infos = {} \n",
    "    for key,value in data_gb:\n",
    "        info = {}\n",
    "        value = value[value['session_time']!=np.nan]\n",
    "        info['session_count'] = len(value)\n",
    "        info['first_time'] = value.session_time.min()\n",
    "        info['last_time'] = value.session_time.max()\n",
    "        info['active_days'] = value.session_time.nunique()\n",
    "        all_infos[key] = info   \n",
    "    df = pd.DataFrame(all_infos).T.reset_index().rename(columns={'index':str(feature)})\n",
    "    data = data.merge(df,how='left',on=str(feature))    \n",
    "    return data  \n",
    "\n",
    "def feature_merge_2023(data,feature):\n",
    "    data_gb = data.groupby(feature)\n",
    "    all_infos = {} \n",
    "    for key,value in data_gb:\n",
    "        info = {}\n",
    "        value = value[value['session_time']!=np.nan]\n",
    "        info['2023_session_count'] = len(value)\n",
    "        info['2023_first_time'] = value.session_time.min()\n",
    "        info['2023_last_time'] = value.session_time.max()\n",
    "        info['2023_active_days'] = value.session_time.nunique()\n",
    "        all_infos[key] = info   \n",
    "    df = pd.DataFrame(all_infos).T.reset_index().rename(columns={'index':str(feature)})\n",
    "    data = data.merge(df,how='left',on=str(feature))    \n",
    "    return data \n",
    "\n",
    "\n",
    "\n",
    "def reduce_mem_usage(data):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = data.memory_usage().sum() \n",
    "    \n",
    "    for col in data.columns:\n",
    "        col_type = data[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = data[col].min()\n",
    "            c_max = data[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    data[col] = data[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    data[col] = data[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    data[col] = data[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    data[col] = data[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    data[col] = data[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    data[col] = data[col].astype(np.float32)\n",
    "                else:\n",
    "                    data[col] = data[col].astype(np.float64)\n",
    "        else:\n",
    "            data[col] = data[col].astype('category')\n",
    "    end_mem = data.memory_usage().sum() \n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
